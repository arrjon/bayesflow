{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# ensure the backend is set\n",
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    # set this to \"torch\", \"tensorflow\", or \"jax\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "\n",
    "# for BayesFlow devs: this ensures that the latest dev version can be found\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import bayesflow as bf\n",
    "from bayesflow.data_adapters import DataAdapter\n",
    "from bayesflow.simulators import make_simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes the meta simulator batched\n",
    "# so we can amortized over the sample size N\n",
    "def meta(batch_shape):\n",
    "  n = np.random.randint(8, 12)\n",
    "  return dict(n=n)\n",
    "\n",
    "def prior():\n",
    "    mu = np.random.normal(0.0, 1.0)\n",
    "    sigma = np.random.gamma(1, 1)\n",
    "    return dict(mu=mu, sigma=sigma)\n",
    "\n",
    "def likelihood(mu, sigma, n):\n",
    "    y = np.random.normal(mu, sigma, size=(n, 1))\n",
    "    return dict(y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perhaps improve the argument name \"meta_fn\"?\n",
    "simulator = bf.simulators.make_simulator([prior, likelihood], meta_fn = meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04879656 -1.53510865]\n",
      "[-0.04879656 -1.53510865]\n"
     ]
    }
   ],
   "source": [
    "# generate a batch of two training samples\n",
    "# TODO: please allow 2 instead of (2, )\n",
    "sample_data = simulator.sample((2, ))\n",
    "print(sample_data[\"mu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_adapter = DataAdapter() \\\n",
    "    .drop([\"dim\"]) \\\n",
    "    .to_array() \\\n",
    "    .broadcast(\"n\") \\\n",
    "    .convert_dtype(from_dtype=\"float64\", to_dtype=\"float32\") \\\n",
    "    .standardize(exclude=\"n\") \\\n",
    "    .concatenate([\"mu\", \"sigma\"], into=\"inference_variables\") \\\n",
    "    .rename(\"n\", \"inference_conditions\") \\\n",
    "    .rename(\"y\", \"summary_variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_network = bf.networks.FlowMatching(\n",
    "    subnet=\"mlp\",\n",
    "    subnet_kwargs=dict(\n",
    "        depth=6,\n",
    "        width=256,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximator = bf.ContinuousApproximator(\n",
    "   inference_network=inference_network,\n",
    "   data_adapter=data_adapter,\n",
    ")\n",
    "\n",
    "# TODO: can we add the data_adapter *only* after creating approximator?\n",
    "# this would shorten the overall code because we wouldn't have to call\n",
    "# bf.ContinuousApproximator twice\n",
    "# TODO: when newly building the data_adapter here we may see an error in .sample later on\n",
    "# approximator.build_data_adapter(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "approximator.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mapproximator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Dropbox/Software/bayesflow/BayesFlow/examples/../bayesflow/approximators/continuous_approximator.py:107\u001b[0m, in \u001b[0;36mContinuousApproximator.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Dropbox/Software/bayesflow/BayesFlow/examples/../bayesflow/approximators/approximator.py:82\u001b[0m, in \u001b[0;36mApproximator.fit\u001b[0;34m(self, dataset, simulator, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     80\u001b[0m     mock_data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m     81\u001b[0m     mock_data \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap_structure(keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor, mock_data)\n",
      "\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m=\u001b[39mdataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[0;32m~/Dropbox/Software/bayesflow/BayesFlow/examples/../bayesflow/approximators/approximator.py:23\u001b[0m, in \u001b[0;36mApproximator.build_from_data\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_from_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28many\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Dropbox/Software/bayesflow/BayesFlow/examples/../bayesflow/approximators/continuous_approximator.py:93\u001b[0m, in \u001b[0;36mContinuousApproximator.compute_metrics\u001b[0;34m(self, inference_variables, inference_conditions, summary_variables, stage)\u001b[0m\n",
      "\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     91\u001b[0m         inference_conditions \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconcatenate([inference_conditions, summary_outputs], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;32m---> 93\u001b[0m inference_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstage\u001b[49m\n",
      "\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m inference_metrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mzeros(())) \u001b[38;5;241m+\u001b[39m summary_metrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mzeros(()))\n",
      "\u001b[1;32m     99\u001b[0m inference_metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/inference_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inference_metrics\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\n",
      "File \u001b[0;32m~/Dropbox/Software/bayesflow/BayesFlow/examples/../bayesflow/networks/flow_matching/flow_matching.py:122\u001b[0m, in \u001b[0;36mFlowMatching.compute_metrics\u001b[0;34m(self, x, conditions, stage)\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m     target_velocity \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m-\u001b[39m x0\n",
      "\u001b[1;32m    120\u001b[0m base_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompute_metrics(x1, conditions, stage)\n",
      "\u001b[0;32m--> 122\u001b[0m predicted_velocity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintegrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvelocity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mmean_squared_error(target_velocity, predicted_velocity)\n",
      "\u001b[1;32m    125\u001b[0m loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmean(loss)\n",
      "\n",
      "File \u001b[0;32m~/Dropbox/Software/bayesflow/BayesFlow/examples/../bayesflow/networks/flow_matching/integrators/euler.py:45\u001b[0m, in \u001b[0;36mEulerIntegrator.velocity\u001b[0;34m(self, x, t, conditions, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     43\u001b[0m     xtc \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconcatenate([x, t], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 45\u001b[0m     xtc \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_projector(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubnet(xtc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/ops/numpy.py:1352\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(xs, axis)\u001b[0m\n",
      "\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(xs):\n",
      "\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Concatenate(axis\u001b[38;5;241m=\u001b[39maxis)\u001b[38;5;241m.\u001b[39msymbolic_call(xs)\n",
      "\u001b[0;32m-> 1352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/miniconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/backend/torch/numpy.py:454\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(xs, axis)\u001b[0m\n",
      "\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate\u001b[39m(xs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;32m    453\u001b[0m     xs \u001b[38;5;241m=\u001b[39m [convert_to_tensor(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n",
      "\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "history = approximator.fit(\n",
    "    epochs=10,\n",
    "    num_batches = 512,\n",
    "    batch_size = 64,\n",
    "    simulator = simulator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of posterior draws you want to get\n",
    "num_samples = 5000\n",
    "\n",
    "# Obtain samples from amortized posterior\n",
    "N = 10\n",
    "lik = likelihood(mu=2.0, sigma=1.0, N = N)\n",
    "# TODO: can we avoid having to specify all the \"None\" dimensions?\n",
    "# TODO: can we stop needing the float32 transformation for pytorch?\n",
    "conditions = {\n",
    "    \"y\": lik[\"y\"],\n",
    "    \"N\": N,\n",
    "}\n",
    "keras.tree.map_structure(keras.ops.shape, conditions)\n",
    "pdraws = approximator.sample(conditions=conditions, num_samples = num_samples)\n",
    "pdraws = np.append(pdraws[\"mu\"], pdraws[\"sigma\"], axis = -1)\n",
    "pdraws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure\n",
    "f, axes = plt.subplots(1, figsize=(6, 6))\n",
    "\n",
    "# Plot samples\n",
    "axes.scatter(pdraws[:, 0], pdraws[:, 1], color=\"#153c7a\", alpha=0.75, s=0.5)\n",
    "sns.despine(ax=axes)\n",
    "axes.set_title(r\"Posterior draws\")\n",
    "axes.grid(alpha=0.3)\n",
    "axes.set_aspect(\"equal\", adjustable=\"box\")\n",
    "axes.set_xlim([0, 4])\n",
    "axes.set_ylim([0, 5])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
